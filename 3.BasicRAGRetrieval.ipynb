{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef40963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175945b5",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a886a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text File\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./data/be-good.txt\")\n",
    "\n",
    "loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6babf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV File\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader('./data/Street_Tree_List.csv')\n",
    "\n",
    "loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HTML\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "loader = UnstructuredHTMLLoader('./data/100-startups.html')\n",
    "\n",
    "loaded_data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185c4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./data/5pages.pdf')\n",
    "\n",
    "loaded_data = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e553ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WIkipedia Page\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "name = \"JFK\"\n",
    "\n",
    "loader = WikipediaLoader(query=name, load_max_docs=1)\n",
    "\n",
    "loaded_data = loader.load()[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e121f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Answer this {question}, here is some extra {context}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    question=\"What was the full name of JFK\",\n",
    "    context=loaded_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40187cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The full name of JFK is John Fitzgerald Kennedy.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 839, 'total_tokens': 849, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'finish_reason': 'stop', 'logprobs': None}, id='run-efbf487c-bd03-4238-b848-961cc30cdd0c-0', usage_metadata={'input_tokens': 839, 'output_tokens': 10, 'total_tokens': 849})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"error\":\"Forbidden\"}\\\\n\\')')\n"
     ]
    }
   ],
   "source": [
    "response = chatModel.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cf236",
   "metadata": {},
   "source": [
    "# Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe43176",
   "metadata": {},
   "source": [
    "## Reminder: steps of the RAG process.\n",
    "* When you load a document, you end up with strings. Sometimes the strings will be too large to fit into the context window. In those occassions we will use the RAG technique:\n",
    "    * **Split document in small chunks**.\n",
    "    * Transform text chunks in numeric chunks (embeddings).\n",
    "    * Load embeddings to a vector database (aka vector store).\n",
    "    * Load question and retrieve the most relevant embeddings to respond it.\n",
    "    * Sent the embeddings to the LLM to format the response properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19133f",
   "metadata": {},
   "source": [
    "## Splitters: divide the loaded document in small chunks of text\n",
    "* Also called \"Document Transformers\".\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n",
    "* See the list of built-in splitters [here](https://python.langchain.com/v0.1/docs/integrations/document_transformers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6eb94f",
   "metadata": {},
   "source": [
    "### Simple splitting by character: Character Splitter\n",
    "* This splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters.\n",
    "* The \"Character Splitter\" in the context of RAG (Retrieval-Augmented Generation) applications, specifically using LangChain's tools, is a method that divides text into smaller parts based on specific characters.\n",
    "* By default, it uses double newline characters (\"\\n\\n\") to identify where one chunk of text ends and another begins.\n",
    "* Each chunk is measured by its number of characters.\n",
    "* This simple splitting method is useful in RAG applications to help manage and process large blocks of text by breaking them down into manageable, smaller pieces. This can enhance the efficiency and effectiveness of the text retrieval process, which is crucial in generating accurate and contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fcc6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./data/be-good.txt\")\n",
    "\n",
    "loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5216d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3346b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([loaded_data[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcb5612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb42f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Be good')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]\n",
    "\n",
    "#texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cad63f",
   "metadata": {},
   "source": [
    "## Splitting with MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814949d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = [{\"this is metadata for first chunk\": 0}, {\"second chunk\": 1}, {\"third chunk\": 3}]\n",
    "\n",
    "documents = text_splitter.create_documents(\n",
    "    [loaded_data[0].page_content, loaded_data[0].page_content], \n",
    "    metadatas=metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2646369",
   "metadata": {},
   "source": [
    "The `CharacterTextSplitter` from LangChain's text splitting utilities helps break down text into manageable chunks along with the option to attach metadata to each chunk. Here’s how it works in simple terms:\n",
    "\n",
    "1. **Initialization of the Splitter**: The splitter is configured with several parameters:\n",
    "   - `separator`: This is the character or string at which the text will be split. In this example, it’s set to `\"\\n\\n\"`, which means the text will be divided at every occurrence of double newlines.\n",
    "   - `chunk_size`: This determines the maximum size of each chunk in characters. Here, each chunk can be up to 1000 characters long.\n",
    "   - `chunk_overlap`: This allows chunks to overlap by a specified number of characters, set here to 200. Overlapping helps ensure that no crucial information is cut off awkwardly at the end of a chunk.\n",
    "   - `length_function`: This is typically a function to measure the length of the text; `len` simply counts the number of characters.\n",
    "   - `is_separator_regex`: This indicates whether the separator is a regular expression or not. Here, it’s `False`, meaning the separator is treated as a literal string.\n",
    "\n",
    "2. **Creating Chunks**: The `create_documents` method is used to split the text. It takes in the text (or texts) and optional metadata, and splits the text based on the defined parameters.\n",
    "\n",
    "3. **Attaching Metadata**: Optionally, metadata can be attached to each chunk to provide additional context or identifiers. For example, metadata might tag each chunk with its sequence number or categorize it based on content. In the example, metadata like `{\"chunk\": 0}` could signify that it’s the first chunk.\n",
    "\n",
    "#### Example Code Explanation:\n",
    "- **First Splitter Use**: Initially, the splitter is used to split the content of `loaded_data[0].page_content` without metadata. This will just divide the text based on the provided settings.\n",
    "- **Second Splitter Use**: The same text is then split again, but this time with metadata provided (`metadatas=[{\"chunk\": 0}, {\"chunk\": 1}]`). This second run distinguishes each chunk with additional information.\n",
    "\n",
    "#### Example Outputs:\n",
    "- **Number of Chunks**: This would typically be printed using `len(texts)`, showing how many chunks the text was split into.\n",
    "- **First Chunk**: By printing `texts[0]`, you would see the content of the first chunk.\n",
    "- **First Chunk with Metadata**: Printing `documents[0]` after the second splitting operation would show the first chunk of text along with its corresponding metadata, illustrating how metadata is associated with text chunks.\n",
    "\n",
    "This method is particularly useful in applications where chunks of text need to be processed independently, but still require some form of contextual or sequential linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d717b6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'this is metadata for first chunk': 0}, page_content='Be good')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d122cac",
   "metadata": {},
   "source": [
    "### Recursive Character Splitter\n",
    "* This text splitter is the recommended one for generic text. \n",
    "* It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. \n",
    "* This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "#### Simple Explanation:\n",
    "\n",
    "* The \"Recursive Character Splitter\" is a method used to divide text into smaller, more manageable chunks, designed specifically to maintain the semantic integrity of the text.\n",
    "* It operates by attempting to split the text using a list of characters in a specified order—beginning with the largest units like paragraphs, then moving to sentences, and finally to individual words if needed.\n",
    "* The default sequence for splitting is [\"\\n\\n\", \"\\n\", \" \", \"\"], which means it first tries to split the text at double newline characters to separate paragraphs, then at single newlines for any remaining large blocks, followed by spaces to isolate sentences or phrases, and finally using an empty string if finer splitting is necessary.\n",
    "* This method is particularly effective because it tries to keep text chunks as meaningful and complete as possible, ensuring that each chunk has a coherent piece of information.\n",
    "\n",
    "#### Example of Use:\n",
    "\n",
    "#### Original Text:\n",
    "```\n",
    "Hello, welcome to our store!\n",
    "\n",
    "\\n\\nWe offer a variety of products. Our range includes electronics, clothing, and home appliances.\n",
    "\\nOur staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
    "```\n",
    "\n",
    "#### Applying Recursive Character Splitter:\n",
    "- First attempt with `\"\\n\\n\"`:\n",
    "  1. **Chunk 1:** `Hello, welcome to our store!`\n",
    "  2. **Chunk 2:** `We offer a variety of products. Our range includes electronics, clothing, and home appliances.\\nOur staff is available to help you during store hours: 9 AM to 9 PM every day.`\n",
    "\n",
    "- Second attempt with `\"\\n\"` for remaining long chunk:\n",
    "  1. **New Chunk 2:** `We offer a variety of products. Our range includes electronics, clothing, and home appliances.`\n",
    "  2. **Chunk 3:** `Our staff is available to help you during store hours: 9 AM to 9 PM every day.`\n",
    "\n",
    "- No further splits are necessary as all chunks are now of manageable size.\n",
    "\n",
    "In this example, the text is initially split into two chunks using the double newline. Since one chunk is still quite long, it then uses a single newline to split it further. Each chunk retains coherent, complete information, reflecting the effective use of the recursive character splitter to preserve the semantic structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e641fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=26,\n",
    "    chunk_overlap=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78d09898",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99523bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"\n",
    "Data that Speak\n",
    "LLM Applications are revolutionizing industries such as \n",
    "banking, healthcare, insurance, education, legal, tourism, \n",
    "construction, logistics, marketing, sales, customer service, \n",
    "and even public administration.\n",
    "\n",
    "The aim of our programs is for students to learn how to \n",
    "create LLM Applications in the context of a business, \n",
    "which presents a set of challenges that are important \n",
    "to consider in advance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f6738fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff20a0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data that Speak',\n",
       " 'LLM Applications are',\n",
       " 'are revolutionizing',\n",
       " 'industries such as',\n",
       " 'banking, healthcare,',\n",
       " 'insurance, education,',\n",
       " 'legal, tourism,',\n",
       " 'construction, logistics,',\n",
       " 'marketing, sales,',\n",
       " 'customer service,',\n",
       " 'and even public',\n",
       " 'administration.',\n",
       " 'The aim of our programs',\n",
       " 'is for students to learn',\n",
       " 'how to',\n",
       " 'create LLM Applications',\n",
       " 'in the context of a',\n",
       " 'a business,',\n",
       " 'which presents a set of',\n",
       " 'of challenges that are',\n",
       " 'are important',\n",
       " 'to consider in advance.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d63bec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f5dce",
   "metadata": {},
   "source": [
    "The separators `[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]` refer to a sequence of characters and patterns used to split text into smaller parts, with each separator serving a specific function based on the structure of text:\n",
    "\n",
    "1. `\"\\n\\n\"` - This separator targets double newlines, which are often used to denote separate paragraphs or sections within a text. Splitting here is intended to separate distinct thematic or topical blocks of content.\n",
    "\n",
    "2. `\"\\n\"` - This targets single newline characters, which typically indicate a new line within a paragraph or a soft break in the content, such as between list items or sub-paragraphs.\n",
    "\n",
    "3. `\"(?<=\\. )\"` - This is a regular expression that looks for a period followed by a space, typically used to signify the end of a sentence. The `(?<= )` part is a \"lookbehind\" assertion in regex, which means it checks for the occurrence of a period and space before splitting, but does not include them in the split, thus keeping sentences intact.\n",
    "\n",
    "4. `\" \"` - This targets single space characters, which are commonly used to separate words. Splitting on spaces can break down text into individual words or phrases, especially useful when finer granularity is needed.\n",
    "\n",
    "5. `\"\"` - An empty string as a separator is used in text processing to split a text into its individual characters, essentially decomposing the text down to its most basic elements.\n",
    "\n",
    "Each of these separators is used to progressively split the text into smaller and smaller chunks, starting from larger structural divisions like paragraphs and moving down to the level of individual characters, depending on the needs of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99b16702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data that Speak\\nLLM Applications are revolutionizing industries such as \\nbanking, healthcare, insurance, education, legal, tourism,',\n",
       " 'construction, logistics, marketing, sales, customer service, \\nand even public administration.',\n",
       " 'The aim of our programs is for students to learn how to \\ncreate LLM Applications in the context of a business,',\n",
       " 'which presents a set of challenges that are important \\nto consider in advance.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_recursive_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd76221",
   "metadata": {},
   "source": [
    "# Embeddings: transform chunks of text in chunks of numbers\n",
    "* Vector databases work very fast with numbers.\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/).\n",
    "* See the list of embedding model providers [here](https://python.langchain.com/v0.1/docs/integrations/text_embedding/).\n",
    "* **The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "312f7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf7413f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_of_text =     [\n",
    "        \"Hi there!\",\n",
    "        \"Hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"Bond, James Bond\",\n",
    "        \"Hello Bond!\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2de00969",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.embed_documents(chunks_of_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e1a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings) # we embed 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb52cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3f90c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45577a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f21c5",
   "metadata": {},
   "source": [
    "# Vector Store\n",
    "* Store embeddings in a very fast searchable database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6eb1",
   "metadata": {},
   "source": [
    "## Reminder: Steps of the RAG process.\n",
    "* When you load a document, you end up with strings. Sometimes the strings will be too large to fit into the context window. In those occassions we will use the RAG technique:\n",
    "    * Split document in small chunks.\n",
    "    * Transform text chunks in numeric chunks (embeddings).\n",
    "    * **Load embeddings to a vector database (aka vector store)**.\n",
    "    * Load question and retrieve the most relevant embeddings to respond it.\n",
    "    * Sent the embeddings to the LLM to format the response properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159fd4b",
   "metadata": {},
   "source": [
    "The `.similarity_search` method in the code is used to find the most relevant text chunks from a pre-processed document that are similar to a given query. Here's how it works step-by-step\n",
    "\n",
    "1. **Document Processing and Embedding:**\n",
    "   - The document `state_of_the_union.txt` is loaded using the `TextLoader` from the LangChain Community package.\n",
    "   - The document is then split into smaller chunks of text using the `CharacterTextSplitter`, where each chunk is 1000 characters long without any overlap between the chunks.\n",
    "   - Each chunk of text is then transformed into an embedding using `OpenAIEmbeddings`. Embeddings are high-dimensional vectors that represent the semantic content of the text.\n",
    "   - These embeddings are stored in an instance of `Chroma`, which serves as a vector database optimized for efficient similarity searches.\n",
    "\n",
    "2. **Using `.similarity_search`:**\n",
    "   - When you invoke `vector_db.similarity_search(question)`, the method converts the query (`\"What did the president say about the John Lewis Voting Rights Act?\"`) into an embedding using the same method that was used for the chunks.\n",
    "   - It then searches the vector database (`vector_db`) to find the chunks whose embeddings are most similar to the embedding of the query. This similarity is typically measured using metrics such as cosine similarity.\n",
    "   - The search results are sorted by relevance, with the most relevant chunks (those that are semantically closest to the query) returned first.\n",
    "\n",
    "3. **Output:**\n",
    "   - The result of the `.similarity_search` is stored in `response`, which contains the relevant chunks and their similarity scores.\n",
    "   - The script prints the content of the most relevant chunk (the first result), which should ideally contain information about what the president said regarding the John Lewis Voting Rights Act.\n",
    "\n",
    "This method is particularly useful in applications like question answering or document retrieval where you need to quickly find the most relevant parts of a large text based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Specify encoding to avoid UnicodeDecodeError\n",
    "loaded_document = TextLoader('data/state_of_the_union.txt', encoding='utf-8').load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "chunks_of_text = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "vector_db = Chroma.from_documents(chunks_of_text, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05839d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "question = \"What did the president say about the John Lewis Voting Rights Act?\"\n",
    "\n",
    "response = vector_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba68baf",
   "metadata": {},
   "source": [
    "# Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527855c",
   "metadata": {},
   "source": [
    "## Vector Stores vs. Retrievers\n",
    "\n",
    "1. **Purpose and Functionality**:\n",
    "   - **Vector Stores**: These are specialized databases designed to store information in the form of vectors (high-dimensional data points that represent text or other information). Vector stores are primarily used for quickly searching and retrieving similar vectors based on a query vector. They are focused on efficiently handling similarity comparisons between the stored vectors and any query vector.\n",
    "   - **Retrievers**: Retrievers are more general tools that use various methods, including vector stores, to find and return relevant documents or information in response to a query. A retriever doesn't necessarily store the information itself but knows how to access and retrieve it when needed.\n",
    "\n",
    "2. **Storage vs. Retrieval**:\n",
    "   - **Vector Stores**: As the name implies, these are primarily concerned with storing data in a structured way that makes it fast and efficient to perform similarity searches.\n",
    "   - **Retrievers**: While they may utilize storage systems like vector stores, retrievers are more focused on the act of fetching the right information in response to a user's query. Their main job is to provide the end-user with the most relevant information or documents based on the input they receive.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - **Vector Stores**: These are somewhat limited in their scope to handling tasks that involve similarity searches within the stored vectors. They are a specific tool for specific types of data retrieval tasks.\n",
    "   - **Retrievers**: They can be designed to use different back-end systems (like vector stores or other databases) and can be part of larger systems that may involve more complex data processing or response generation.\n",
    "\n",
    "In summary, vector stores in LangChain are about how information is stored and efficiently accessed based on similarity, while retrievers are about using various methods (including vector stores) to actively fetch and return the right information in response to diverse queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cd58c",
   "metadata": {},
   "source": [
    "## Retriever: returns a response given a question\n",
    "\n",
    "1. **Retriever: returns a response given a question** - A retriever is a tool that provides specific pieces of information or documents when you ask a question or make a query.\n",
    "\n",
    "2. **A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.** - A retriever works by taking a question that doesn't have a fixed format (an unstructured query) and finding relevant documents based on that question. It's a broad tool, more versatile than a vector store, which is just one way to organize information to make retrieval efficient.\n",
    "\n",
    "3. **A retriever does not need to be able to store documents, only to return (or retrieve) them.** - The main job of a retriever is to find and return documents when asked; it doesn't have to store these documents itself. It can find documents stored elsewhere.\n",
    "\n",
    "4. **Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.** - Although many retrievers use a system called a vector store to help find the right documents quickly (a vector store organizes information into a format that's easy to search through), there are other ways to build a retriever that don't rely on this method.\n",
    "\n",
    "Overall, a retriever helps you find information you need from a large amount of data by searching through documents, and it can use different methods to do this efficiently.\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/).\n",
    "* See the list of third-party retrievers [here](https://python.langchain.com/v0.1/docs/integrations/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ff992",
   "metadata": {},
   "source": [
    "## Differences .similarity_search vs. .as_retriever()\n",
    "Both methods involve finding the most relevant text based on a query, but they are structured differently and may offer different functionalities based on their implementation.\n",
    "\n",
    "#### `.similarity_search`\n",
    "\n",
    "This method directly performs a similarity search against a vector database, which in your first code snippet is managed by the `Chroma` class. The process includes:\n",
    "- Embedding the input query using the same model that was used to embed the document chunks.\n",
    "- Searching the vector database for the closest vectors to the query's embedding.\n",
    "- Returning the most relevant chunks based on their semantic similarity to the query.\n",
    "\n",
    "This method is straightforward and typically used when you need to quickly find and retrieve the text segments that best match the query.\n",
    "\n",
    "#### `.as_retriever()`\n",
    "\n",
    "This method involves a different approach:\n",
    "1. **Retriever Setup**: In the second code snippet, `vector_db.as_retriever()` converts the vector database (managed by `FAISS` in this case) into a retriever object. This object abstracts the similarity search into a retriever model that can be used in more complex retrieval-augmented generation (RAG) tasks.\n",
    "2. **Invoke Method**: The `invoke()` function on the retriever is then used to perform the query. This method can be part of a larger system where the retriever is integrated with other components (like a language model) to generate answers or further process the retrieved documents.\n",
    "\n",
    "#### Key Differences\n",
    "\n",
    "- **Flexibility**: `.as_retriever()` provides a more flexible interface that can be integrated into larger, more complex systems, potentially combining retrieval with generation (like in RAG setups). This method is beneficial in applications where the retrieved content might be used as input for further processing or answer generation.\n",
    "- **Backend Implementation**: While `.similarity_search` directly accesses the vector database, `.as_retriever()` encapsulates this access within a retriever object, which might have additional functionalities or optimizations for specific retrieval tasks.\n",
    "- **Use Cases**: The direct `.similarity_search` might be faster and more straightforward for simple query-to-document retrieval tasks. In contrast, `.as_retriever()` could be used in scenarios requiring additional steps after retrieval, like feeding the retrieved information into a language model for generating coherent and context-aware responses.\n",
    "\n",
    "Both methods are useful, but their appropriateness depends on the specific requirements of your application, such as whether you need straightforward retrieval or a more complex retrieval-augmented generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7350062",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba76c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple use case without LCEL\n",
    "response = retriever.invoke(\"what did he say about ketanji brown jackson?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37e30bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ab57ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/state_of_the_union.txt'}, page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92163bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/state_of_the_union.txt'}, page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'),\n",
       " Document(metadata={'source': 'data/state_of_the_union.txt'}, page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.'),\n",
       " Document(metadata={'source': 'data/state_of_the_union.txt'}, page_content='But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \\n\\nDanielle says Heath was a fighter to the very end. \\n\\nHe didn’t know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle—we are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers. \\n\\nI’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \\n\\nAnd fourth, let’s end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America–second only to heart disease.'),\n",
       " Document(metadata={'source': 'data/state_of_the_union.txt'}, page_content='One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I’m committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter.')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4e717",
   "metadata": {},
   "source": [
    "# Top K\n",
    "* Decide how many embeddings will be retrieved to build the answer to your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27e180ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "question = \"What did the president say about the John Lewis Voting Rights Act?\"\n",
    "\n",
    "response = vector_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289a939",
   "metadata": {},
   "source": [
    "## Retriever: returns a response given a question\n",
    "* A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.\n",
    "* A retriever does not need to be able to store documents, only to return (or retrieve) them.\n",
    "* Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/).\n",
    "* See the list of third-party retrievers [here](https://python.langchain.com/v0.1/docs/integrations/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8958f91",
   "metadata": {},
   "source": [
    "## Specify Top K\n",
    "\n",
    "Here's a simple explanation of what's happening:\n",
    "\n",
    "- **`search_kwargs={\"k\": 1}`**: This parameter is being set when the retriever object is created from the vector database. Here, `k` stands for the number of results or documents to retrieve that are most similar to the query. By setting `k` to 1, the code specifies that the retriever should return only the single most relevant document or chunk of text in response to a query. \n",
    "\n",
    "So, when you use this parameter:\n",
    "- **Purpose**: It tells the retriever to limit the number of retrieved documents to the top 1 most relevant result.\n",
    "- **Usage**: This is particularly useful when you are only interested in the very best match for a query and not in other closely related matches.\n",
    "\n",
    "In the example, when the retriever is invoked with the query about what was said regarding \"ketanji brown jackson\", it will only return the single most relevant piece of information or document from the database, rather than providing several possible answers.\n",
    "\n",
    "This approach is useful for applications where clarity and precision are more valuable than breadth, such as when you need a concise answer to a specific question without additional context that might be slightly less relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75b504be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "response = retriever.invoke(\"what did he say about ketanji brown jackson?\")\n",
    "\n",
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2029f22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/state_of_the_union.txt'}, page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed07bbd",
   "metadata": {},
   "source": [
    "# RAG with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ccf7e",
   "metadata": {},
   "source": [
    "## Simple use with LCEL and input and output formatters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d5d5f",
   "metadata": {},
   "source": [
    "Here’s what each part of the chain does, in simple terms:\n",
    "\n",
    "1. **Retrieving Relevant Documents**: First, the system retrieves documents that are relevant to the question you ask. It uses a retriever which is designed to find the most relevant information based on the query.\n",
    "\n",
    "2. **Formatting Documents**: Once the relevant documents are retrieved, the next step is to format them into a readable format. The function `format_docs` does this by taking the content of each document and separating them with two newline spaces. This creates a clear and organized chunk of text that serves as the context for answering the question.\n",
    "\n",
    "3. **Preparing the Prompt**: With the context formatted, the system uses a template to prepare the prompt for the AI model. The template structures the input by placing the formatted context first and then the question. This way, the AI model knows exactly what the background information is and what it needs to answer.\n",
    "\n",
    "4. **Generating the Answer**: The structured prompt is then fed into an AI model, in this case, ChatOpenAI. The AI reads the combined context and question and generates an answer based on what it knows from the provided text.\n",
    "\n",
    "5. **Extracting the Final Answer**: Finally, the response from the AI is parsed into a straightforward text format using `StrOutputParser`, making it easy to read and understand. This step ensures that what you get as the output is just the answer, cleaned up from any formatting or raw data that the AI might output.\n",
    "\n",
    "In essence, this code automates the process of fetching relevant information, preparing it, and asking an AI to provide a clear answer based on that information. It's like setting up a mini question-answering system where the AI has all the necessary context to provide accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "220bcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"what did he say about ketanji brown jackson?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2b4c332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He referred to Ketanji Brown Jackson as one of the nation's top legal minds and stated that she will continue Justice Breyer’s legacy of excellence.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
